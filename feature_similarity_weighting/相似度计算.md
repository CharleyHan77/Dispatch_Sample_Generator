 # 四种特征相似度计算逻辑与实现详解

## 概述

本文档详细分析了 `feature_similarity_weighting/calculate_weighted_similarity.py` 中四种特征的相似度计算逻辑与实现细节。该脚本的核心目标是**对新FJSP数据实例与历史数据集进行多特征融合的相似度检索**，实现对新数据的"最相似历史样本"推荐。

## 四种特征相似度计算详解

### 1. 基础特征相似度计算

**特征内容：**
- `num_jobs`：工件数量
- `num_machines`：机器数量  
- `total_operations`：总操作数
- `avg_available_machines`：平均可用机器数
- `std_available_machines`：可用机器数标准差

**计算逻辑：**
```python
# 1. 特征标准化（Z-score标准化）
normalized_features = normalize_features(all_features)

# 2. 欧氏距离计算
basic_distance = calculate_euclidean_distance(
    normalized_features[new_data_file]["basic_features"],
    normalized_features[hist_file]["basic_features"]
)

# 3. 距离归一化为相似度
basic_similarity = normalize_distance(basic_distance, max_basic_distance)
```

**实现细节：**
- **标准化处理**：使用Z-score标准化，消除不同特征量纲影响
- **距离度量**：采用欧氏距离，计算两个5维特征向量的几何距离
- **相似度转换**：通过 `1 - (distance / max_distance)` 将距离转换为[0,1]区间的相似度
- **权重分配**：在最终加权中占30%权重

**特点：**
- 反映FJSP实例的基本规模特征
- 标准化确保不同规模实例间的可比性
- 欧氏距离对特征差异敏感，适合捕捉整体相似性

---

### 2. 加工时间特征相似度计算

**特征内容：**
- `processing_time_mean`：加工时间均值
- `processing_time_std`：加工时间标准差
- `processing_time_min`：加工时间最小值
- `processing_time_max`：加工时间最大值
- `machine_time_variance`：机器时间方差

**计算逻辑：**
```python
# 1. 特征标准化（与基础特征相同）
normalized_features = normalize_features(all_features)

# 2. 欧氏距离计算
processing_distance = calculate_euclidean_distance(
    normalized_features[new_data_file]["processing_time_features"],
    normalized_features[hist_file]["processing_time_features"]
)

# 3. 距离归一化为相似度
processing_similarity = normalize_distance(processing_distance, max_processing_distance)
```

**实现细节：**
- **标准化处理**：同样使用Z-score标准化
- **距离度量**：采用欧氏距离，衡量加工时间分布特征的差异
- **相似度转换**：通过最大距离归一化转换为相似度
- **权重分配**：在最终加权中占25%权重

**特点：**
- 反映FJSP实例的加工时间分布特征
- 捕捉时间复杂度和调度难度
- 对加工时间模式敏感，有助于识别相似的时间特征

---

### 3. KDE特征相似度计算

**特征内容：**
- `x_grid`：密度估计的网格点
- `density`：对应网格点的密度值
- `bandwidth`：核密度估计的带宽参数

**计算逻辑：**
```python
# 使用JS散度计算分布相似度
kde_similarity = 1 - calculate_js_divergence(
    new_data_features[new_data_file]["kde_features"]["density"],
    historical_features[hist_file]["kde_features"]["density"]
)
```

**实现细节：**
- **分布度量**：采用Jensen-Shannon散度（JS散度），衡量两个概率分布的差异
- **相似度转换**：通过 `1 - JS散度` 转换为相似度
- **数值稳定性**：添加小常数避免零值，处理无穷大情况
- **权重分配**：在最终加权中占20%权重

**JS散度计算：**
```python
def calculate_js_divergence(p, q):
    # 确保概率分布和为1
    p = np.array(p) + epsilon
    q = np.array(q) + epsilon
    p = p / np.sum(p)
    q = q / np.sum(q)
    
    # 计算平均分布
    m = 0.5 * (p + q)
    
    # 计算JS散度
    js_div = 0.5 * (entropy(p, m) + entropy(q, m))
    
    return js_div if not np.isinf(js_div) else 1.0
```

**特点：**
- 反映加工时间分布的连续特征
- JS散度对分布形状敏感，能捕捉复杂的分布模式
- 相比简单统计量，能更好地表达时间分布的相似性

---

### 4. 析取图特征相似度计算

**特征内容：**
- `nodes_count`：节点数量
- `edges_count`：边数量
- `solid_frequency`：实线边标签频率
- `dashed_frequency`：虚线边标签频率

**计算逻辑：**
```python
def calculate_disjunctive_graph_similarity(graph_info1, graph_info2):
    # 1. 结构相似度计算
    nodes_similarity = 1 - abs(nodes1 - nodes2) / max(nodes1, nodes2)
    edges_similarity = 1 - abs(edges1 - edges2) / max(edges1, edges2)
    structure_similarity = (nodes_similarity + edges_similarity) / 2
    
    # 2. 标签相似度计算
    # 实线标签Jaccard相似度
    solid_jaccard = len(solid_keys1.intersection(solid_keys2)) / len(solid_keys1.union(solid_keys2))
    
    # 实线标签余弦相似度
    solid_cosine = np.dot(solid_vec1, solid_vec2) / (solid_norm1 * solid_norm2)
    
    # 虚线标签类似计算...
    
    # 3. 综合相似度
    solid_similarity = 0.3 * solid_jaccard + 0.7 * solid_cosine
    dashed_similarity = 0.3 * dashed_jaccard + 0.7 * dashed_cosine
    label_similarity = 0.6 * solid_similarity + 0.4 * dashed_similarity
    
    # 4. 最终加权
    weighted_similarity = 0.5 * structure_similarity + 0.5 * label_similarity
    
    return weighted_similarity
```

**实现细节：**

**结构相似度：**
- 基于节点数和边数的相对差异
- 使用 `1 - |a-b|/max(a,b)` 计算相对相似度
- 结构相似度 = (节点相似度 + 边相似度) / 2

**标签相似度：**
- **Jaccard相似度**：衡量标签集合的重叠程度
- **余弦相似度**：衡量标签频率分布的相似性
- **加权组合**：Jaccard权重0.3，余弦权重0.7
- **边类型权重**：实线权重0.6，虚线权重0.4

**最终加权：**
- 结构相似度权重：50%
- 标签相似度权重：50%
- 在最终综合加权中占25%权重

**特点：**
- 反映FJSP实例的拓扑结构特征
- 结合了图的基本结构信息和WL标签信息
- 通过权重调整平衡结构和标签信息的重要性

---

## 综合加权相似度计算

**权重分配：**
```python
weighted_similarity = (
    0.3 * basic_similarity +      # 基础特征：30%
    0.25 * processing_similarity + # 加工时间：25%
    0.2 * kde_similarity +        # KDE特征：20%
    0.25 * disjunctive_similarity # 析取图：25%
)
```

**设计考虑：**
- 基础特征权重最高，反映FJSP实例的基本属性
- 析取图特征权重较高，体现结构相似性的重要性
- 加工时间特征权重适中，平衡统计特征和分布特征
- KDE特征权重相对较低，作为分布特征的补充

---

## 计算流程总结

1. **数据预处理**：加载特征数据，进行标准化处理
2. **最大距离计算**：第一遍遍历计算各类特征的最大距离
3. **相似度计算**：第二遍遍历计算每个历史样本的四类特征相似度
4. **加权融合**：按权重组合四类特征相似度
5. **结果排序**：按综合相似度降序排列
6. **结果输出**：保存详细结果和可视化图表

---

## 应用场景与优势

**应用场景：**
- 新FJSP实例的"最相似历史样本"推荐与检索
- 数据集特征分布分析与可视化
- 支持后续调度算法推荐、迁移学习等智能决策模块

**优势：**
- **多维度特征融合**：从不同角度全面评估FJSP实例的相似性
- **标准化处理**：确保不同量纲特征的可比性
- **权重可调**：可根据实际需求灵活调整各特征权重
- **高效计算**：支持大规模历史数据集的批量相似度计算
- **结果可视化**：提供详细的相似度对比图表，便于分析

这种多特征融合的方法能够从不同角度全面评估FJSP实例的相似性，为后续的调度算法推荐和迁移学习提供可靠的相似度基础。